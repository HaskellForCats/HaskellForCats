Over the last 50 years, Moore’s observation — that the number of transistors on silicon chips and therefore their processing power was doubling approximately every 24 months — has evolved from observation to market demand to Moore’s law.

But Moore’s law is running out of gas. In 2012 for example, the cost per silicon transistor started to climb due to the investment required for each new, more powerful generation of chip. Further, silicon has reached its performance limitations, failing to fuel innovation at the speed to which we have become accustomed.

"Moore's law" is the observation that the number of transistors in a dense integrated circuit has doubled approximately every two years. The observation is named after Gordon E. Moore, the co-founder of Intel and Fairchild Semiconductor, whose 1965 paper described a doubling every year in the number of components per integrated circuit,[note 1] and projected this rate of growth would continue for at least another decade.[2][3][4] In 1975,[5] looking forward to the next decade,[6] he revised the forecast to doubling every two years.[7][8][9]

Although the rate held steady from 1975 until around 2012, the rate was faster during the first decade. In general, it is not logically sound to extrapolate from the historical growth rate into the indefinite future.
Intel confirmed in 2015 that the pace of advancement has slowed,

The Multicore Era
While manufacturers were forced to stop increasing CPU clock speeds, Moore’s Law continued without interruption, and by 2005, manufacturers could fit all of a CPU’s functionality into half of a CPU’s area. What to do with the vacated space? Add the functionality of a second CPU! And so in 2006, dual-core CPUs appeared, in which one CPU chip contained the core functionality of two traditional CPUs. By 2008, quad-core CPUs were available, in which a CPU contained the functionality of four traditional CPUs. Since then, this trend has continued, as Moore’s Law has let manufacturers produce CPUs containing more and more cores. As examples, AMD’s Interlagos, Abu Dhabi, and Delhi  processor lines offer 4, 8, 12, and 16-core models; Intel’s Xeon E7-88xx series offers 4, 10, 16, and 18-core models, and its Xeon Phi coprocessor currently offers up to 61 cores, with a 72-core standalone model expected this year.

 Developers who are still writing sequential software are writing for yesterday’s hardware foundation, not today’s; and CS programs that teach their students to think solely in terms of sequential computing are doing their students a disservice. (Joel C. Adams )

As a result of this shift, sequential processors have been replaced by parallel multiprocessors. Other factors being equal, a dual-core CPU has twice the computational potential of a single-core CPU, a quad-core CPU has four times the potential of a single-core CPU, and so on.

The problem is that a traditional, sequential program will only use one of a multicore processor’s cored; that is, running a traditional, sequential program on a quad-core CPU only uses 25% of the potential of that CPU; 75% of that potential is squandered. To take advantage of a multicore CPU’s potential, software must be re-engineered using parallel computing techniques.

Prior to 2006, parallel computing was a niche area, mainly for researchers and specialists in high performance computing. As a result, parallel (or high performance) computing was an elective area in the 2001 ACM/IEEE CS Curriculum, and relatively few universities offered undergraduate courses on the subject.

However, with advent of the multicore era in 2006, parallel multiprocessors suddenly became inexpensive. Virtually all CPUs today are multicore CPUs, and because of this ubiquity, all computer science graduates need at least a basic understanding of parallel computing. Put differently, multiprocessors are the de facto hardware foundation on which virtually all of today’s software runs. Developers who are still writing sequential software are writing for yesterday’s hardware foundation, not today’s; and CS programs that teach their students to think solely in terms of sequential computing are doing their students a disservice.

In recognition of this seismic shift in the computing landscape, the ACM/IEEE CS Curriculum 2013 (CS 2013) contains a new knowledge area named Parallel and Distributed Computing (PDC), with 15 hours of topics in the core CS curriculum. The Systems Fundamentals knowledge area also includes additional hours of PDC topics.

It is worth mentioning that PDC topics extend well beyond the concurrency-related topics (multithreading, synchronization primitives, monitors, etc.) that have been in the core CS curriculum for years. The new PDC knowledge area includes topics like performance, speedup, computational efficiency, scalability, Amdahl’s law, and so on. (Joel C. Adams @ http://cacm.acm.org/)
